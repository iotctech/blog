---
title: 数据增强综述
date: 2020-07-27 09:50:54
tags:
 - 深度学习
 - 数据增强
 - Data Augmentation
categories:
 - [deeplearning]
---

## 1、概述
很多实际的项目，我们都难以有充足的数据来完成任务，要保证完美的完成任务，有两件事情需要做好：

- 寻找更多的数据
- 数据增强（data augmentation）

**什么是数据增强**
数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值。

![](1.jpg)

数据增强可以分为**有监督的数据增强和无监督的数据增强方法**。其中有监督的数据增强又可以分为**单样本数据增强和多样本数据增强方法，无监督的数据增强分为生成新的数据和学习增强策略两个方向**。

## 2、有监督的数据增强
有监督数据增强，即**采用预设的数据变换规则，在已有数据的基础上进行数据的扩增**，包含单样本数据增强和多样本数据增强。

### 2.1、单样本数据增强
单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括**几何变换类，颜色变换类等**。

#### 2.1.1、几何变换类
几何变换类即对图像进行几何变换，包括**翻转，旋转，裁剪，变形，缩放**等各类操作.

> **翻转**

翻转包括水平翻转和垂直翻转。

![](2.webp)

> **随机裁剪**

裁剪图片的感兴趣区域（ROI），通常在训练的时候，会采用随机裁剪的方法，下图为随机裁剪4次的效果。

![](3.webp)

> **随机旋转**

对图像做一定角度对旋转操作.

![](4.webp)

> **缩放变形**

随机选取图像的一部分，然后将其缩放到原图像尺度。

![](5.webp)

翻转操作和旋转操作，对于那些对方向不敏感的任务，比如图像分类，都是很常见的操作，在caffe等框架中翻转对应的就是mirror操作。

翻转和旋转不改变图像的大小，而裁剪会改变图像的大小。通常在训练的时候会采用随机裁剪的方法，在测试的时候选择裁剪中间部分或者不裁剪。值得注意的是，在一些竞赛中进行模型测试时，一般都是裁剪输入的多个版本然后将结果进行融合，对预测的改进效果非常明显。

以上操作都不会产生失真，而缩放变形则是失真的。

很多的时候，网络的训练输入大小是固定的，但是数据集中的图像却大小不一，此时就可以选择上面的裁剪成固定大小输入或者缩放到网络的输入大小的方案，后者就会产生失真，通常效果比前者差。

> **仿射变换(Affine Transformation)**

同时对图片做裁剪、旋转、转换、模式调整等多重操作。

Affine Transformation是一种二维坐标到二维坐标之间的线性变换，保持二维图形的“平直性”（译注：straightness，即变换后直线还是直线不会打弯，圆弧还是圆弧）和“平行性”（译注：parallelness，其实是指保二维图形间的相对位置关系不变，平行线还是平行线，相交直线的交角不变。）

![](7.png)

仿射变换可以通过一系列的原子变换的复合来实现，包括：平移（Translation）、缩放（Scale）、翻转（Flip）、旋转（Rotation）和剪切（Shear）。

![](6.webp)

> **透视变换**

透视变换（Perspective Transformation）的本质是将图像投影到一个新的视平面，也称作投影映射。它是二维（x,y）到三维(X,Y,Z)，再到另一个二维(x’,y’)空间的映射。

仿射变换后平行四边形的各边仍操持平行，透视变换结果允许是梯形等四边形，所以仿射变换是透视变换的子集。

对图像应用一个随机的四点透视变换。

![](8.webp)

#### 2.1.2、颜色变换类
上面的几何变换类操作，没有改变图像本身的内容，它可能是选择了图像的一部分或者对像素进行了重分布。如果要改变图像本身的内容，就属于颜色变换类的数据增强了，常见的包括**噪声、模糊、颜色变换、擦除、填充**等等。

基于噪声的数据增强就是在原来的图片的基础上，随机叠加一些噪声，最常见的做法就是**高斯噪声**。更复杂一点的就是在面积大小可选定、位置随机的矩形区域上丢弃像素产生黑色矩形块，从而产生一些彩色噪声，以Coarse Dropout方法为代表，甚至还可以对图片上随机选取一块区域并擦除图像信息。

**噪声类：**

> **高斯噪声**

高斯噪声是指它的概率密度函数服从高斯分布（即正态分布）的一类噪声。如果一个噪声，它的幅度分布服从高斯分布，而它的功率谱密度又是均匀分布的，则称它为高斯白噪声。高斯白噪声的二阶矩不相关，一阶矩为常数，是指先后信号在时间上的相关性。

概率密度函数:

![](10.png)

σ为z的标准差,z为均值

![](9.webp)

> **椒盐噪声**

椒盐噪声是根据图像的信噪比,随机生成一些图像内的像素位置,并随机对这些像素点赋值为0或255。

概率密度函数:

![](11.png)

> **CoarseDropout**

在面积大小可选定、位置随机的矩形区域上丢失信息实现转换，所有通道的信息丢失产生黑色矩形块，部分通道的信息丢失产生彩色噪声。

![](12.webp)

> **SimplexNoiseAlpha**

产生连续单一噪声的掩模后，将掩模与原图像混合

![](13.webp)

> **FrequencyNoiseAlpha**

在频域中用随机指数对噪声映射进行加权，再转换到空间域。在不同图像中，随着指数值逐渐增大，依次出现平滑的大斑点、多云模式、重复出现的小斑块

![](14.webp)

**模糊类：**

减少各像素点值的差异实现图片模糊，实现像素的平滑化。

> **高斯模糊**

![](15.webp)

> **ElasticTransformation**

根据扭曲场的平滑度与强度逐一地移动局部像素点实现模糊效果。

![](16.webp)

> **HSV对比度变换**

通过向HSV空间中的每个像素添加或减少V值，修改色调和饱和度实现对比度转换。

![](17.webp)

> **RGB颜色扰动**

将图片从RGB颜色空间转换到另一颜色空间，增加或减少颜色参数后返回RGB颜色空间。

![](18.webp)

> **随机擦除法**

对图片上随机选取一块区域，随机地擦除图像信息。

![](19.webp)

> **超像素法（Superpixels）**

在最大分辨率处生成图像的若干个超像素，并将其调整到原始大小，再将原始图像中所有超像素区域按一定比例替换为超像素，其他区域不改变。

![](20.webp)

> **转换法（invert）**

按给定的概率值将部分或全部通道的像素值从v设置为255-v。

![](21.webp)

> **边界检测（EdgeDetect）**

检测图像中的所有边缘，将它们标记为黑白图像，再将结果与原始图像叠加。

![](22.webp)

> **GrayScale**

将图像从RGB颜色空间转换为灰度空间，通过某一通道与原图像混合。

![](23.webp)

> **锐化（sharpen）与浮雕（emboss）**

对图像执行某一程度的锐化或浮雕操作，通过某一通道将结果与图像融合。

下图分别是锐化与浮雕效果图:

![](24.webp)

### 2.2、多样本数据增强
不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法:

#### 2.2.1、SMOTE
SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。

类不平衡现象是很常见的，它指的是数据集中各类别数量不近似相等。如果样本类别之间相差很大，会影响分类器的分类效果。假设小样本数据数量极少，如仅占总体的1%，则即使小样本被错误地全部识别为大样本，在经验风险最小化策略下的分类器识别准确率仍能达到99%，但由于没有学习到小样本的特征，实际分类效果就会很差。

SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为：

- 第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N；

- 第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点，满足以下公式：

![](25.jpg)

- 第三步，重复以上的步骤，直到大、小样本数量平衡。

在python中，SMOTE算法已经封装到了imbalanced-learn库中，如下图为算法实现的数据增强的实例，左图为原始数据特征空间图，右图为SMOTE算法处理后的特征空间图:

![](26.webp)

#### 2.2.2、SamplePairing
SamplePairing方法的原理非常简单，从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。

![](27.webp)

训练过程是交替禁用与使用SamplePairing处理操作的结合：

- 使用传统的数据增强训练网络，不使用SamplePairing 数据增强训练。

- 在ILSVRC数据集上完成一个epoch或在其他数据集上完成100个epoch后，加入SamplePairing 数据增强训练。

- 间歇性禁用 SamplePairing。对于 ILSVRC 数据集，为其中的300000 个图像启用SamplePairing，然后在接下来的100000个图像中禁用它。对于其他数据集，在开始的8个epoch中启用，在接下来的2个epoch中禁止。

- 在训练损失函数和精度稳定后进行微调，禁用SamplePairing。

经SamplePairing处理后可使训练集的规模从N扩增到N×N。实验结果表明，因SamplePairing数据增强操作可能引入不同标签的训练样本，导致在各数据集上使用SamplePairing训练的误差明显增加，而在验证集上误差则有较大幅度降低。

尽管SamplePairing思路简单，性能上提升效果可观，符合奥卡姆剃刀原理，但遗憾的是可解释性不强。

#### 2.2.3、mixup
mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。

令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据，则数据生成方式如下:

![](28.jpg)

λ的取指范围介于0到1。提出mixup方法的作者们做了丰富的实验，实验结果表明可以改进深度学习模型在ImageNet数据集、CIFAR数据集、语音数据集和表格数据集中的泛化误差，降低模型对已损坏标签的记忆，增强模型对对抗样本的鲁棒性和训练生成对抗网络的稳定性。

**SMOTE，SamplePairing，mixup三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布**，不过所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。如果能够在给定范围之外适当插值，也许能实现更好的数据增强效果。

## 3、无监督的数据增强
无监督的数据增强方法包括两类：

- 通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN
- 通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment

### 3.1、GAN
关于GAN(generative adversarial networks)，它包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下：

- G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z) 。
- D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。

### 3.2、Autoaugmentation
AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法。

## 4、参考代码
```
train_parameters = {
    "input_size": [3, 224, 224],
    "image_enhance_strategy": {  # 图像增强相关策略
        "need_distort": True,  # 是否启用图像颜色增强
        "need_rotate": True,   # 是否需要增加随机角度
        "need_crop": True,      # 是否要增加裁剪
        "need_flip": True,      # 是否要增加水平随机翻转
        "hue_prob": 0.5,
        "hue_delta": 18,
        "contrast_prob": 0.5,
        "contrast_delta": 0.5,
        "saturation_prob": 0.5,
        "saturation_delta": 0.5,
        "brightness_prob": 0.5,
        "brightness_delta": 0.125
    }
}
```

```
# 数据处理
def resize_img(img, target_size):
    """
    强制缩放图片
    :param img:
    :param target_size:
    :return:
    """
    target_size = input_size
    img = img.resize((target_size[1], target_size[2]), Image.BILINEAR)
    return img


def random_crop(img, scale=[0.08, 1.0], ratio=[3. / 4., 4. / 3.]):
    aspect_ratio = math.sqrt(np.random.uniform(*ratio))
    w = 1. * aspect_ratio
    h = 1. / aspect_ratio

    bound = min((float(img.size[0]) / img.size[1]) / (w**2),
                (float(img.size[1]) / img.size[0]) / (h**2))
    scale_max = min(scale[1], bound)
    scale_min = min(scale[0], bound)

    target_area = img.size[0] * img.size[1] * np.random.uniform(scale_min,
                                                                scale_max)
    target_size = math.sqrt(target_area)
    w = int(target_size * w)
    h = int(target_size * h)

    i = np.random.randint(0, img.size[0] - w + 1)
    j = np.random.randint(0, img.size[1] - h + 1)

    img = img.crop((i, j, i + w, j + h))
    img = img.resize((train_parameters['input_size'][1], train_parameters['input_size'][2]), Image.BILINEAR)
    return img


def rotate_image(img):
    """
    图像增强，增加随机旋转角度
    """
    angle = np.random.randint(-14, 15)
    img = img.rotate(angle)
    return img


def random_brightness(img):
    """
    图像增强，亮度调整
    :param img:
    :return:
    """
    prob = np.random.uniform(0, 1)
    if prob < train_parameters['image_enhance_strategy']['brightness_prob']:
        brightness_delta = train_parameters['image_enhance_strategy']['brightness_delta']
        delta = np.random.uniform(-brightness_delta, brightness_delta) + 1
        img = ImageEnhance.Brightness(img).enhance(delta)
    return img


def random_contrast(img):
    """
    图像增强，对比度调整
    :param img:
    :return:
    """
    prob = np.random.uniform(0, 1)
    if prob < train_parameters['image_enhance_strategy']['contrast_prob']:
        contrast_delta = train_parameters['image_enhance_strategy']['contrast_delta']
        delta = np.random.uniform(-contrast_delta, contrast_delta) + 1
        img = ImageEnhance.Contrast(img).enhance(delta)
    return img


def random_saturation(img):
    """
    图像增强，饱和度调整
    :param img:
    :return:
    """
    prob = np.random.uniform(0, 1)
    if prob < train_parameters['image_enhance_strategy']['saturation_prob']:
        saturation_delta = train_parameters['image_enhance_strategy']['saturation_delta']
        delta = np.random.uniform(-saturation_delta, saturation_delta) + 1
        img = ImageEnhance.Color(img).enhance(delta)
    return img


def random_hue(img):
    """
    图像增强，色度调整
    :param img:
    :return:
    """
    prob = np.random.uniform(0, 1)
    if prob < train_parameters['image_enhance_strategy']['hue_prob']:
        hue_delta = train_parameters['image_enhance_strategy']['hue_delta']
        delta = np.random.uniform(-hue_delta, hue_delta)
        img_hsv = np.array(img.convert('HSV'))
        img_hsv[:, :, 0] = img_hsv[:, :, 0] + delta
        img = Image.fromarray(img_hsv, mode='HSV').convert('RGB')
    return img


def distort_color(img):
    """
    概率的图像增强
    :param img:
    :return:
    """
    prob = np.random.uniform(0, 1)
    # Apply different distort order
    if prob < 0.35:
        img = random_brightness(img)
        img = random_contrast(img)
        img = random_saturation(img)
        img = random_hue(img)
    elif prob < 0.7:
        img = random_brightness(img)
        img = random_saturation(img)
        img = random_hue(img)
        img = random_contrast(img)
    return img
```

```
# 数据增强策略：

# 1、在线模式--训练中
# 随机裁剪(完全随机，四个角+中心)  crop

def random_crop(img, scale=[0.8, 1.0], ratio=[3. / 4., 4. / 3.], resize_w=100, resize_h=100):
    """
    随机裁剪
    :param img:
    :param scale: 缩放
    :param ratio:
    :param resize_w:
    :param resize_h:
    :return:
    """
    aspect_ratio = math.sqrt(np.random.uniform(*ratio))
    w = 1. * aspect_ratio
    h = 1. / aspect_ratio
    src_h, src_w = img.shape[:2]
 
    bound = min((float(src_w) / src_h) / (w ** 2),
                (float(src_h) / src_w) / (h ** 2))
    scale_max = min(scale[1], bound)
    scale_min = min(scale[0], bound)
 
    target_area = src_h * src_w * np.random.uniform(scale_min, scale_max)
    target_size = math.sqrt(target_area)
    w = int(target_size * w)
    h = int(target_size * h)
 
    i = np.random.randint(0, src_w - w + 1)
    j = np.random.randint(0, src_h - h + 1)
 
    img = img[j:j + h, i:i + w]
    img = cv2.resize(img, (resize_w, resize_h))
    return img
 
 
def rule_crop(img, box_ratio=(3. / 4, 3. / 4), location_type='LT', resize_w=100, resize_h=100):
    """
    按照一定规则进行裁剪, 直接在原图尺寸上操作，不对原图进行
    :param img:
    :param box_ratio: 剪切的 比例：  （宽度上的比例， 高度上的比例）
    :param location_type: 具体在=哪个位置： 以下其中一个：
        LR : 左上角
        RT : 右上角
        LB : 左下角
        RB : 右下角
        CC : 中心
    :param resize_w: 输出图的width
    :param resize_h: 输出图的height
    :return:
    """
    assert location_type in ('LT', 'RT', 'LB', 'RB', 'CC'), 'must have a location .'
    is_gray = False
    if len(img.shape) == 3:
        h, w, c = img.shape
    elif len(img.shape) == 2:
        h, w = img.shape
        is_gray = True
 
    crop_w, crop_h = int(w * box_ratio[0]), int(h * box_ratio[1])
    crop_img = np.zeros([10, 10])
    if location_type == 'LT':
        crop_img = img[:crop_h, :crop_w, :] if not is_gray else img[:crop_h, :crop_w]
    elif location_type == 'RT':
        crop_img = img[:crop_h:, w - crop_w:, :] if not is_gray else img[:crop_h:, w - crop_w:]
    elif location_type == 'LB':
        crop_img = img[h - crop_h:, :crop_w, :] if not is_gray else img[h - crop_h:, :crop_w]
    elif location_type == 'RB':
        crop_img = img[h - crop_h:, w - crop_w:, :] if not is_gray else img[h - crop_h:, w - crop_w:]
    elif location_type == 'CC':
        start_h = (h - crop_h) // 2
        start_w = (w - crop_w) // 2
        crop_img = img[start_h:start_h + crop_h, start_w:start_w + crop_w, :] if not is_gray else img[
                                                                                                    start_h:start_h + crop_h,
                                                                                                    start_w:start_w + crop_w]
 
    resize = cv2.resize(crop_img, (resize_w, resize_h))
    return resize


# 水平翻转  flip
def random_flip(img, mode=1):
    """
    随机翻转
    :param img:
    :param model: 1=水平翻转 / 0=垂直 / -1=水平垂直
    :return:
    """
    assert mode in (0, 1, -1), "mode is not right"
    flip = np.random.choice(2) * 2 - 1  # -1 / 1
    if mode == 1:
        img = img[:, ::flip, :]
    elif mode == 0:
        img = img[::flip, :, :]
    elif mode == -1:
        img = img[::flip, ::flip, :]
 
    return img
 
 
def flip(img, mode=1):
    """
    翻转
    :param img:
    :param mode: 1=水平翻转 / 0=垂直 / -1=水平垂直
    :return:
    """
    assert mode in (0, 1, -1), "mode is not right"
    return cv2.flip(img, flipCode=mode)


# 随机锐化增强
def random_USM(img, gamma=0.):
    """
    USM锐化增强算法可以去除一些细小的干扰细节和图像噪声，比一般直接使用卷积锐化算子得到的图像更可靠。
        output = 原图像−w∗高斯滤波(原图像)/(1−w)
        其中w为上面所述的系数，取值范围为0.1~0.9，一般取0.6。
    :param img:
    :param gamma:
    :return:
    """
    blur = cv2.GaussianBlur(img, (0, 0), 25)
    img_sharp = cv2.addWeighted(img, 1.5, blur, -0.3, gamma)
    return img_sharp


# 2 离线模式
# 2.1 随机扰动

# 噪声(高斯、自定义)  noise
def random_noise(img, rand_range=(3, 20)):
    """
    随机噪声
    :param img:
    :param rand_range: (min, max)
    :return:
    """
    img = np.asarray(img, np.float)
    sigma = random.randint(*rand_range)
    nosie = np.random.normal(0, sigma, size=img.shape)
    img += nosie
    img = np.uint8(np.clip(img, 0, 255))
    return img
 

# 滤波(高斯、平滑、均值、中值、最大最小值、双边、引导、运动)
# 各种滤波原理介绍：https://blog.csdn.net/hellocsz/article/details/80727972
def gaussianBlue(img, ks=(7, 7), stdev=1.5):
    """
    高斯模糊, 可以对图像进行平滑处理，去除尖锐噪声
    :param img:
    :param ks:  卷积核
    :param stdev: 标准差
    :return:
    """
    return cv2.GaussianBlur(img, (7, 7), 1.5)


# 随机滤波
def ranndom_blur(img, ksize=(3, 3)):
    """
    随机滤波
    :param img:
    :param ksize:
    :return:
    """
    blur_types = ['gaussian', 'median', 'bilateral', 'mean', 'box']
    assert len(blur_types) > 0
    blur_func = None
    blur_index = random.choice(blur_types)
    if blur_index == 0:  # 高斯模糊, 比均值滤波更平滑，边界保留更加好
        blur_func = cv2.GaussianBlur
    elif blur_index == 1:  # 中值滤波, 在边界保存方面好于均值滤波，但在模板变大的时候会存在一些边界的模糊。对于椒盐噪声有效
        blur_func = cv2.medianBlur
    elif blur_index == 2:  # 双边滤波, 非线性滤波，保留较多的高频信息，不能干净的过滤高频噪声，对于低频滤波较好，不能去除脉冲噪声
        blur_func = cv2.bilateralFilter
    elif blur_index == 3:  # 均值滤波, 在去噪的同时去除了很多细节部分，将图像变得模糊
        blur_func = cv2.blur
    elif blur_index == 4:  # 盒滤波器
        blur_func = cv2.boxFilter
 
    img_blur = blur_func(src=img, ksize=ksize)
    return img_blur


# 直方图均衡化
def equalize_hist(img):
    """
    直方图均衡化
    :param img:
    :return:
    """
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    hist = cv2.equalizeHist(gray)
    rgb = cv2.cvtColor(hist, cv2.COLOR_GRAY2RGB)
    return rgb


# 2.2 转换
# 旋转  rorate
def rotate(img, angle, scale=1.0):
    """
    旋转
    :param img:
    :param angle: 旋转角度， >0 表示逆时针，
    :param scale:
    :return:
    """
    height, width = img.shape[:2]  # 获取图像的高和宽
    center = (width / 2, height / 2)  # 取图像的中点
 
    M = cv2.getRotationMatrix2D(center, angle, scale)  # 获得图像绕着某一点的旋转矩阵
    # cv2.warpAffine()的第二个参数是变换矩阵,第三个参数是输出图像的大小
    rotated = cv2.warpAffine(img, M, (height, width))
    return rotated
 
 
def random_rotate(img, angle_range=(-10, 10)):
    """
    随机旋转
    :param img:
    :param angle_range:  旋转角度范围 (min,max)   >0 表示逆时针，
    :return:
    """
    height, width = img.shape[:2]  # 获取图像的高和宽
    center = (width / 2, height / 2)  # 取图像的中点
    angle = random.randrange(*angle_range, 1)
    M = cv2.getRotationMatrix2D(center, angle, 1.0)  # 获得图像绕着某一点的旋转矩阵
    # cv2.warpAffine()的第二个参数是变换矩阵,第三个参数是输出图像的大小
    rotated = cv2.warpAffine(img, M, (height, width))
    return rotated


# 偏移  shift
def shift(img, x_offset, y_offset):
    """
    偏移，向右 向下
    :param img:
    :param x_offset:  >0表示向右偏移px, <0表示向左
    :param y_offset:  >0表示向下偏移px, <0表示向上
    :return:
    """
    h, w, _ = img.shape
    M = np.array([[1, 0, x_offset], [0, 1, y_offset]], dtype=np.float)
    return cv2.warpAffine(img, M, (w, h))


# 倾斜  skew
# ...
# 缩放  scale
def resize_img(img, resize_w, resize_h):
    height, width = img.shape[:2]  # 获取图片的高和宽
    return cv2.resize(img, (resize_w, resize_h), interpolation=cv2.INTER_CUBIC)


# RGB/BGR->HSV
def rgb2hsv_py(r, g, b):
    # from https://blog.csdn.net/weixin_43360384/article/details/84871521
    r, g, b = r/255.0, g/255.0, b/255.0
    mx = max(r, g, b)
    mn = min(r, g, b)
    m = mx-mn
    if mx == mn:
        h = 0
    elif mx == r:
        if g >= b:
            h = ((g-b)/m)*60
        else:
            h = ((g-b)/m)*60 + 360
    elif mx == g:
        h = ((b-r)/m)*60 + 120
    elif mx == b:
        h = ((r-g)/m)*60 + 240
    elif mx == 0:
        s = 0
    else:
        s = m/mx
    v = mx
    return h, s, v


def rgb2hsv_cv(img):
    # from https://blog.csdn.net/qq_38332453/article/details/89258058
    h = img.shape[0]
    w = img.shape[1]
    H = np.zeros((h,w),np.float32)
    S = np.zeros((h, w), np.float32)
    V = np.zeros((h, w), np.float32)
    r,g,b = cv2.split(img)
    r, g, b = r/255.0, g/255.0, b/255.0
    for i in range(0, h):
        for j in range(0, w):
            mx = max((b[i, j], g[i, j], r[i, j]))
            mn = min((b[i, j], g[i, j], r[i, j]))
            V[i, j] = mx
            if V[i, j] == 0:
                S[i, j] = 0
            else:
                S[i, j] = (V[i, j] - mn) / V[i, j]
            if mx == mn:
                H[i, j] = 0
            elif V[i, j] == r[i, j]:
                if g[i, j] >= b[i, j]:
                    H[i, j] = (60 * ((g[i, j]) - b[i, j]) / (V[i, j] - mn))
                else:
                    H[i, j] = (60 * ((g[i, j]) - b[i, j]) / (V[i, j] - mn))+360
            elif V[i, j] == g[i, j]:
                H[i, j] = 60 * ((b[i, j]) - r[i, j]) / (V[i, j] - mn) + 120
            elif V[i, j] == b[i, j]:
                H[i, j] = 60 * ((r[i, j]) - g[i, j]) / (V[i, j] - mn) + 240
            H[i,j] = H[i,j] / 2
    return H, S, V


# 图片叠加与融合
def addWeight(src1, alpha, src2, beta, gamma):
    """
    g (x) = (1 − α)f0 (x) + αf1 (x)   #a→（0，1）不同的a值可以实现不同的效果
    dst = src1 * alpha + src2 * beta + gamma
    :param src1: img1
    :param alpha:
    :param src2: img2
    :param beta:
    :param gamma:
    :return:
    """
    assert src1.shap == src2.shape
    return cv2.addWeighted(src1, alpha, src2, beta, gamma)


# 颜色抖动(亮度\色度\饱和度\对比度)  color jitter
def adjust_contrast_bright(img, contrast=1.2, brightness=100):
    """
    调整亮度与对比度
    dst = img * contrast + brightness
    :param img:
    :param contrast: 对比度   越大越亮
    :param brightness: 亮度  0~100
    :return:
    """
    # 像素值会超过0-255， 因此需要截断
    return np.uint8(np.clip((contrast * img + brightness), 0, 255))


def pytorch_color_jitter(img):
    return torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)


# gamma 变换
def gamma_transform(img, gamma=1.0):
    """
    https://blog.csdn.net/zfjBIT/article/details/85113946
    伽马变换就是用来图像增强，其提升了暗部细节，简单来说就是通过非线性变换，
    让图像从暴光强度的线性响应变得更接近人眼感受的响应，即将漂白（相机曝光）或过暗（曝光不足）的图片，进行矫正
    :param img:
    :param gamma:
        # gamma = random.random() * random.choice([0.5, 1, 3, 5])
        >1, 变暗
        <1, 漂白
    :return:
    """
    assert 0 < gamma < 25.
    # 具体做法先归一化到1，然后gamma作为指数值求出新的像素值再还原
    gamma_table = [np.power(x / 255.0, gamma) * 255.0 for x in range(256)]
    gamma_table = np.round(np.array(gamma_table)).astype(np.uint8)
    # 实现映射用的是Opencv的查表函数
    return cv2.LUT(img, gamma_table)


# mix up 图片混合
def mixup(batch_x, batch_y, alpha):
    """
    Returns mixed inputs, pairs of targets, and lambda
    :param batch_x:
    :param batch_y:
    :param alpha:
    :return:
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
 
    batch_size = batch_x.shape[0]
    index = [i for i in range(batch_size)]
    random.shuffle(index)
 
    mixed_x = lam * batch_x + (1 - lam) * batch_x[index, :]
    y_a, y_b = batch_y, batch_y[index]
    return mixed_x, y_a, y_b, lam
```